{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ibrahimalhurani/Dataset_and_Code/blob/main/AE_CTGAN_BLCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85580357",
      "metadata": {
        "id": "85580357"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your multiomics data\n",
        "rna_data = pd.read_csv('mGE.csv')\n",
        "dna_data = pd.read_csv('mDM.csv')\n",
        "cna_data = pd.read_csv('mCNA.csv')\n",
        "\n",
        "# Assuming 'id' is the common identifier\n",
        "common_ids = set(rna_data['SAMPLE_ID']).intersection(dna_data['SAMPLE_ID'], cna_data['SAMPLE_ID'])\n",
        "\n",
        "# Filter data to include only common samples\n",
        "rna_data = rna_data[rna_data['SAMPLE_ID'].isin(common_ids)]\n",
        "dna_data = dna_data[dna_data['SAMPLE_ID'].isin(common_ids)]\n",
        "cna_data = cna_data[cna_data['SAMPLE_ID'].isin(common_ids)]\n",
        "\n",
        "# Extract features and class labels\n",
        "X_train_rna = rna_data.iloc[:, 1:].values  # Features\n",
        "X_train_dna = dna_data.iloc[:, 1:].values  # Features\n",
        "X_train_cna = cna_data.iloc[:, 1:].values  # Features\n",
        "\n",
        "# Assuming the class labels are in the 'class_label' column of the RNA dataset\n",
        "y_train = rna_data['CLASS'].values\n",
        "\n",
        "# Define and train autoencoders for each data type\n",
        "def build_autoencoder(input_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_dim=input_dim))\n",
        "    model.add(layers.Dense(input_dim, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='mse')  # Use mean squared error for reconstruction loss\n",
        "    return model\n",
        "\n",
        "autoencoder_rna = build_autoencoder(X_train_rna.shape[1])\n",
        "autoencoder_rna.fit(X_train_rna, X_train_rna, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "autoencoder_dna = build_autoencoder(X_train_dna.shape[1])\n",
        "autoencoder_dna.fit(X_train_dna, X_train_dna, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "autoencoder_cna = build_autoencoder(X_train_cna.shape[1])\n",
        "autoencoder_cna.fit(X_train_cna, X_train_cna, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Extract latent space representations\n",
        "latent_rna = autoencoder_rna.predict(X_train_rna)\n",
        "latent_dna = autoencoder_dna.predict(X_train_dna)\n",
        "latent_cna = autoencoder_cna.predict(X_train_cna)\n",
        "\n",
        "# Concatenate latent spaces\n",
        "latent_space = np.concatenate((latent_rna, latent_dna, latent_cna), axis=1)\n",
        "\n",
        "# Automatically determine the minority class\n",
        "class_counts = np.unique(y_train, return_counts=True)\n",
        "minority_class = class_counts[0][np.argmin(class_counts[1])]\n",
        "\n",
        "# ... (rest of the code) ...\n",
        "# Separate majority and minority class samples\n",
        "majority_samples = latent_space[y_train != minority_class]\n",
        "minority_samples = latent_space[y_train == minority_class]\n",
        "\n",
        "# Upsample the minority class\n",
        "upsampled_minority = resample(minority_samples, replace=True, n_samples=len(majority_samples), random_state=42)\n",
        "\n",
        "# Combine the upsampled minority class with the majority class\n",
        "X_train_upsampled = np.vstack([majority_samples, upsampled_minority])\n",
        "y_train_upsampled = np.concatenate([np.zeros(len(majority_samples)), np.ones(len(upsampled_minority))])\n",
        "\n",
        "# Shuffle the upsampled data\n",
        "shuffle_idx = np.random.permutation(len(X_train_upsampled))\n",
        "X_train_upsampled = X_train_upsampled[shuffle_idx]\n",
        "y_train_upsampled = y_train_upsampled[shuffle_idx]\n",
        "\n",
        "# Convert data to float32\n",
        "X_train_upsampled = X_train_upsampled.astype('float32')\n",
        "\n",
        "# Define the Generator network\n",
        "def build_generator(latent_dim, output_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, input_dim=latent_dim, activation='relu'))\n",
        "    model.add(layers.Dense(output_dim, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator network\n",
        "def build_discriminator(input_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Compile the discriminator model\n",
        "discriminator = build_discriminator(X_train_upsampled.shape[1])\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the GAN model\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = models.Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model\n",
        "\n",
        "# Build and compile the generator model\n",
        "latent_dim = 100\n",
        "generator = build_generator(latent_dim, X_train_upsampled.shape[1])\n",
        "\n",
        "# Compile the GAN model\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Continue with GAN training and generating synthetic samples\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Select a random batch of real data for the minority class\n",
        "    idx_minority = np.random.choice(np.where(y_train_upsampled == 1)[0], batch_size // 2, replace=True)\n",
        "    real_samples_minority = X_train_upsampled[idx_minority]\n",
        "    real_labels_minority = y_train_upsampled[idx_minority]\n",
        "\n",
        "    # Generate synthetic samples for the minority class\n",
        "    noise_minority = np.random.normal(0, 1, (batch_size // 2, latent_dim))\n",
        "    generated_samples_minority = generator.predict(noise_minority)\n",
        "    generated_labels_minority = np.ones((batch_size // 2,))\n",
        "\n",
        "    # Train the discriminator on real and generated samples for the minority class\n",
        "    d_loss_real_minority = discriminator.train_on_batch(real_samples_minority, real_labels_minority)\n",
        "    d_loss_generated_minority = discriminator.train_on_batch(generated_samples_minority, generated_labels_minority)\n",
        "\n",
        "    # Train the generator to fool the discriminator for the minority class\n",
        "    noise_minority = np.random.normal(0, 1, (batch_size // 2, latent_dim))\n",
        "    misleading_labels_minority = np.zeros((batch_size // 2,))\n",
        "    g_loss_minority = gan.train_on_batch(noise_minority, misleading_labels_minority)\n",
        "\n",
        "    # Print progress for the minority class\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, D Loss Real Minority: {d_loss_real_minority[0]}, D Loss Generated Minority: {d_loss_generated_minority[0]}, G Loss Minority: {g_loss_minority}\")\n",
        "\n",
        "# Generate synthetic samples after training for the minority class\n",
        "num_samples_minority = len(X_train_upsampled) // 2\n",
        "noise_minority = np.random.normal(0, 1, (num_samples_minority, latent_dim))\n",
        "synthetic_samples_minority = generator.predict(noise_minority)\n",
        "\n",
        "# Combine synthetic samples for the minority class with\n",
        "# Combine synthetic samples with the original data\n",
        "augmented_data = np.vstack([X_train_upsampled, synthetic_samples_minority])\n",
        "augmented_labels = np.concatenate([y_train_upsampled, np.ones(len(synthetic_samples_minority))])\n",
        "\n",
        "# Shuffle the augmented data\n",
        "shuffle_idx = np.random.permutation(len(augmented_data))\n",
        "augmented_data = augmented_data[shuffle_idx]\n",
        "augmented_labels = augmented_labels[shuffle_idx]\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Collect cross-validation results\n",
        "cv_results = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'accuracy': [],\n",
        "    'auc_roc': [],\n",
        "    'fpr': [],\n",
        "    'tpr': []\n",
        "}\n",
        "\n",
        "for train_index, test_index in kf.split(augmented_data):  # New loop for cross-validation\n",
        "    X_train_eval, X_test_eval = augmented_data[train_index], augmented_data[test_index]\n",
        "    y_train_eval, y_test_eval = augmented_labels[train_index], augmented_labels[test_index]\n",
        "\n",
        "    # Define and compile your neural network model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_dim=X_train_eval.shape[1]))\n",
        "    model.add(layers.Dropout(0.2))  # Adding dropout with a dropout rate of 0.2\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Adding EarlyStopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    prediction_model_epochs = 30\n",
        "\n",
        "    # Train the model and collect metrics for plotting\n",
        "    history = model.fit(X_train_eval, y_train_eval, epochs=prediction_model_epochs, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "    # Extract metrics\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    train_acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    # Save metrics for this fold\n",
        "    cv_results['train_loss'].append(train_loss)\n",
        "    cv_results['val_loss'].append(val_loss)\n",
        "    cv_results['train_acc'].append(train_acc)\n",
        "    cv_results['val_acc'].append(val_acc)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred_eval = model.predict(X_test_eval)\n",
        "    y_pred_binary_eval = (y_pred_eval > 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy_eval = accuracy_score(y_test_eval, y_pred_binary_eval)\n",
        "    auc_roc_eval = roc_auc_score(y_test_eval, y_pred_eval)\n",
        "    fpr, tpr, _ = roc_curve(y_test_eval, y_pred_eval)\n",
        "\n",
        "    # Save evaluation metrics\n",
        "    cv_results['accuracy'].append(accuracy_eval)\n",
        "    cv_results['auc_roc'].append(auc_roc_eval)\n",
        "    cv_results['fpr'].append(fpr)\n",
        "    cv_results['tpr'].append(tpr)\n",
        "\n",
        "# Calculate average metrics across folds\n",
        "avg_train_loss = np.mean([np.min(losses) for losses in cv_results['train_loss']])\n",
        "avg_val_loss = np.mean([np.min(losses) for losses in cv_results['val_loss']])\n",
        "avg_train_acc = np.mean([np.max(accs) for accs in cv_results['train_acc']])\n",
        "avg_val_acc = np.mean([np.max(accs) for accs in cv_results['val_acc']])\n",
        "avg_accuracy = np.mean(cv_results['accuracy'])\n",
        "avg_auc_roc = np.mean(cv_results['auc_roc'])\n",
        "\n",
        "# Print averaged cross-validation results\n",
        "print(f'Average Training Loss: {avg_train_loss}')\n",
        "print(f'Average Validation Loss: {avg_val_loss}')\n",
        "print(f'Average Training Accuracy: {avg_train_acc}')\n",
        "print(f'Average Validation Accuracy: {avg_val_acc}')\n",
        "print(f'Average Accuracy: {avg_accuracy}')\n",
        "print(f'Average AUC-ROC: {avg_auc_roc}')\n",
        "\n",
        "# Generate the plots as in the original code\n",
        "\n",
        "# Plot training and validation loss for the last fold\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(cv_results['train_loss'][-1]) + 1), cv_results['train_loss'][-1], label='Training Loss')\n",
        "plt.plot(range(1, len(cv_results['val_loss'][-1]) + 1), cv_results['val_loss'][-1], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss (Last Fold)')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation accuracy for the last fold\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(cv_results['train_acc'][-1]) + 1), cv_results['train_acc'][-1], label='Training Accuracy')\n",
        "plt.plot(range(1, len(cv_results['val_acc'][-1]) + 1), cv_results['val_acc'][-1], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy (Last Fold)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Combine FPR and TPR from all folds for a single ROC curve\n",
        "all_fpr = np.unique(np.concatenate([cv_results['fpr'][i] for i in range(10)]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(10):\n",
        "    mean_tpr += np.interp(all_fpr, cv_results['fpr'][i], cv_results['tpr'][i])\n",
        "mean_tpr /= 10\n",
        "\n",
        "roc_auc = auc(all_fpr, mean_tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(all_fpr, mean_tpr, color='darkorange', lw=2, label='Mean ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61dc4288",
      "metadata": {
        "id": "61dc4288"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "#from joblib import Parallel, delayed  # For parallel processing (if available)\n",
        "#import dask.dataframe as dd  # For efficient data loading (if needed)\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "\n",
        "# Load data (using Dask if needed)\n",
        "# If your datasets are large, replace pd.read_csv with dd.read_csv\n",
        "rna_data = pd.read_csv('mGE.csv')\n",
        "dna_data = pd.read_csv('mDM.csv')\n",
        "cna_data = pd.read_csv('mCNA.csv')\n",
        "\n",
        "# Find common IDs\n",
        "common_ids = set(rna_data['SAMPLE_ID']).intersection(dna_data['SAMPLE_ID'], cna_data['SAMPLE_ID'])\n",
        "\n",
        "# Filter data to include only common samples\n",
        "rna_data = rna_data[rna_data['SAMPLE_ID'].isin(common_ids)]\n",
        "dna_data = dna_data[dna_data['SAMPLE_ID'].isin(common_ids)]\n",
        "cna_data = cna_data[cna_data['SAMPLE_ID'].isin(common_ids)]\n",
        "\n",
        "# Extract features and class labels\n",
        "X_train_rna = rna_data.iloc[:, 1:].values\n",
        "X_train_dna = dna_data.iloc[:, 1:].values\n",
        "X_train_cna = cna_data.iloc[:, 1:].values\n",
        "y_train = rna_data['CLASS'].values\n",
        "\n",
        "\n",
        "# --- 2. Autoencoder Training ---\n",
        "\n",
        "def build_autoencoder(input_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_dim=input_dim))\n",
        "    model.add(layers.Dense(input_dim, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Create and train autoencoders (adjust epochs and batch_size as needed)\n",
        "autoencoder_rna = build_autoencoder(X_train_rna.shape[1])\n",
        "autoencoder_rna.fit(X_train_rna, X_train_rna, epochs=30, batch_size=64, validation_split=0.2)\n",
        "\n",
        "autoencoder_dna = build_autoencoder(X_train_dna.shape[1])\n",
        "autoencoder_dna.fit(X_train_dna, X_train_dna, epochs=30, batch_size=64, validation_split=0.2)\n",
        "\n",
        "autoencoder_cna = build_autoencoder(X_train_cna.shape[1])\n",
        "autoencoder_cna.fit(X_train_cna, X_train_cna, epochs=30, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Extract latent space representations\n",
        "latent_rna = autoencoder_rna.predict(X_train_rna)\n",
        "latent_dna = autoencoder_dna.predict(X_train_dna)\n",
        "latent_cna = autoencoder_cna.predict(X_train_cna)\n",
        "\n",
        "# Concatenate latent spaces\n",
        "latent_space = np.concatenate((latent_rna, latent_dna, latent_cna), axis=1)\n",
        "\n",
        "\n",
        "# --- 3. Data Augmentation with GAN ---\n",
        "\n",
        "# --- Adjustments in GAN Training ---\n",
        "\n",
        "# Hyperparameter tuning (experiment with these values)\n",
        "latent_dim = 128\n",
        "batch_size = 128\n",
        "epochs_gan = 200\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- 4. Cross-Validation and Evaluation ---\n",
        "\n",
        "# Initialize KFold (adjust n_splits as needed)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Collect cross-validation results\n",
        "cv_results = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'accuracy': [],\n",
        "    'auc_roc': [],\n",
        "    'fpr': [],\n",
        "    'tpr': []\n",
        "}\n",
        "\n",
        "\n",
        "# --- Cross-Validation Loop ---\n",
        "for train_index, test_index in kf.split(augmented_data):\n",
        "    X_train_eval, X_test_eval = augmented_data[train_index], augmented_data[test_index]\n",
        "    y_train_eval, y_test_eval = augmented_labels[train_index], augmented_labels[test_index]\n",
        "\n",
        "    # Define and compile the prediction model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_dim=X_train_eval.shape[1]))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train_eval, y_train_eval, epochs=30, batch_size=32,\n",
        "                        validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "    # Store metrics\n",
        "    cv_results['train_loss'].append(history.history['loss'])\n",
        "    cv_results['val_loss'].append(history.history['val_loss'])\n",
        "    cv_results['train_acc'].append(history.history['accuracy'])\n",
        "    cv_results['val_acc'].append(history.history['val_accuracy'])\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred_eval = model.predict(X_test_eval)\n",
        "    y_pred_binary_eval = (y_pred_eval > 0.5).astype(int)\n",
        "\n",
        "    # Calculate and store evaluation metrics\n",
        "    accuracy_eval = accuracy_score(y_test_eval, y_pred_binary_eval)\n",
        "    auc_roc_eval = roc_auc_score(y_test_eval, y_pred_eval)\n",
        "    fpr, tpr, _ = roc_curve(y_test_eval, y_pred_eval)\n",
        "\n",
        "    cv_results['accuracy'].append(accuracy_eval)\n",
        "    cv_results['auc_roc'].append(auc_roc_eval)\n",
        "    cv_results['fpr'].append(fpr)\n",
        "    cv_results['tpr'].append(tpr)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ptvxWLqHtzQf"
      },
      "id": "ptvxWLqHtzQf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cross-Validation Loop ---\n",
        "for train_index, test_index in kf.split(augmented_data):\n",
        "    X_train_eval, X_test_eval = augmented_data[train_index], augmented_data[test_index]\n",
        "    y_train_eval, y_test_eval = augmented_labels[train_index], augmented_labels[test_index]\n",
        "\n",
        "    # Define and compile the prediction model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_dim=X_train_eval.shape[1]))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train_eval, y_train_eval, epochs=30, batch_size=32,\n",
        "                        validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "    # Store metrics\n",
        "    cv_results['train_loss'].append(history.history['loss'])\n",
        "    cv_results['val_loss'].append(history.history['val_loss'])\n",
        "    cv_results['train_acc'].append(history.history['accuracy'])\n",
        "    cv_results['val_acc'].append(history.history['val_accuracy'])\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred_eval = model.predict(X_test_eval)\n",
        "    y_pred_binary_eval = (y_pred_eval > 0.5).astype(int)\n",
        "\n",
        "    # Calculate and store evaluation metrics\n",
        "    accuracy_eval = accuracy_score(y_test_eval, y_pred_binary_eval)\n",
        "    auc_roc_eval = roc_auc_score(y_test_eval, y_pred_eval)\n",
        "    fpr, tpr, _ = roc_curve(y_test_eval, y_pred_eval)\n",
        "\n",
        "    cv_results['accuracy'].append(accuracy_eval)\n",
        "    cv_results['auc_roc'].append(auc_roc_eval)\n",
        "    cv_results['fpr'].append(fpr)\n",
        "    cv_results['tpr'].append(tpr)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3bJ9MJ5ct7xe"
      },
      "id": "3bJ9MJ5ct7xe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Calculate and Print Average Metrics ---\n",
        "\n",
        "avg_train_loss = np.mean([np.min(losses) for losses in cv_results['train_loss']])\n",
        "avg_val_loss = np.mean([np.min(losses) for losses in cv_results['val_loss']])\n",
        "avg_train_acc = np.mean([np.max(accs) for accs in cv_results['train_acc']])\n",
        "avg_val_acc = np.mean([np.max(accs) for accs in cv_results['val_acc']])\n",
        "avg_accuracy = np.mean(cv_results['accuracy'])\n",
        "avg_auc_roc = np.mean(cv_results['auc_roc'])\n",
        "\n",
        "print(f'Average Training Loss: {avg_train_loss}')\n",
        "print(f'Average Validation Loss: {avg_val_loss}')\n",
        "print(f'Average Training Accuracy: {avg_train_acc}')\n",
        "print(f'Average Validation Accuracy: {avg_val_acc}')\n",
        "print(f'Average Accuracy: {avg_accuracy}')\n",
        "print(f'Average AUC-ROC: {avg_auc_roc}')\n",
        "\n",
        "\n",
        "# --- Generate Plots ---\n",
        "\n",
        "# Plot training and validation loss for the last fold\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(cv_results['train_loss'][-1]) + 1), cv_results['train_loss'][-1], label='Training Loss')\n",
        "plt.plot(range(1, len(cv_results['val_loss'][-1]) + 1), cv_results['val_loss'][-1], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss (Last Fold)')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation accuracy for the last fold\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(cv_results['train_acc'][-1]) + 1), cv_results['train_acc'][-1], label='Training Accuracy')\n",
        "plt.plot(range(1, len(cv_results['val_acc'][-1]) + 1), cv_results['val_acc'][-1], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy (Last Fold)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Combine FPR and TPR from all folds for a single ROC curve\n",
        "all_fpr = np.unique(np.concatenate([cv_results['fpr'][i] for i in range(kf.n_splits)]))  # Use kf.n_splits\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(kf.n_splits):  # Use kf.n_splits\n",
        "    mean_tpr += np.interp(all_fpr, cv_results['fpr'][i], cv_results['tpr'][i])\n",
        "mean_tpr /= kf.n_splits  # Use kf.n_splits\n",
        "\n",
        "roc_auc = auc(all_fpr, mean_tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(all_fpr, mean_tpr, color='darkorange', lw=2, label='Mean ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XaPbdeT6u8bj"
      },
      "id": "XaPbdeT6u8bj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine FPR and TPR from all folds for a single ROC curve with more points\n",
        "all_fpr = np.unique(np.concatenate([cv_results['fpr'][i] for i in range(kf.n_splits)]))\n",
        "\n",
        "# Interpolate to get more points on the ROC curve\n",
        "num_points = 100  # You can adjust this number for more/fewer points\n",
        "base_fpr = np.linspace(0, 1, num_points)\n",
        "\n",
        "mean_tpr = np.zeros_like(base_fpr)\n",
        "for i in range(kf.n_splits):\n",
        "    mean_tpr += np.interp(base_fpr, cv_results['fpr'][i], cv_results['tpr'][i])\n",
        "mean_tpr /= kf.n_splits\n",
        "\n",
        "roc_auc = auc(base_fpr, mean_tpr) # Calculate AUC using interpolated points\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(base_fpr, mean_tpr, color='darkorange', lw=2, label='Mean ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0bMRwTEfvOz_"
      },
      "id": "0bMRwTEfvOz_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sdv\n",
        "from sdv.single_table import CTGANSynthesizer\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "\n",
        "\n",
        "# --- 3. Data Augmentation with CTGAN ---\n",
        "\n",
        "# Fit CTGAN to the minority class data in the LATENT SPACE\n",
        "# Assuming 'class' is your target column and minority_class is defined\n",
        "minority_data_latent = pd.DataFrame(latent_space[y_train == minority_class])\n",
        "\n",
        "# Create metadata and define NO primary key - latent space has no id\n",
        "metadata = SingleTableMetadata()\n",
        "metadata.detect_from_dataframe(data=minority_data_latent)\n",
        "#metadata.set_primary_key(column_name='id') # Replace with the actual primary key column if needed\n",
        "\n",
        "# Create a CTGAN synthesizer\n",
        "ctgan = CTGANSynthesizer(metadata=metadata,\n",
        "                         epochs=300,        # Adjust as needed\n",
        "                         batch_size=500     # Adjust as needed\n",
        ")\n",
        "\n",
        "ctgan.fit(minority_data_latent)\n",
        "\n",
        "# Generate synthetic samples\n",
        "num_synthetic_samples = len(majority_samples)  # Generate as many as the majority class\n",
        "synthetic_data = ctgan.sample(num_synthetic_samples)\n",
        "\n",
        "# Extract features from synthetic data - NO NEED: already in latent space\n",
        "#synthetic_features = synthetic_data.iloc[:, 1:].values  # Assuming features start from column 1\n",
        "\n",
        "# Combine synthetic samples with original data\n",
        "augmented_data = np.vstack([latent_space, synthetic_data])  # Combine with latent space data\n",
        "augmented_labels = np.concatenate([y_train, np.ones(num_synthetic_samples)])  # Add labels for synthetic data\n",
        "\n",
        "# Shuffle augmented data\n",
        "shuffle_idx = np.random.permutation(len(augmented_data))\n",
        "augmented_data = augmented_data[shuffle_idx]\n",
        "augmented_labels = augmented_labels[shuffle_idx]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tG_pJdRFvsYR"
      },
      "id": "tG_pJdRFvsYR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sdv\n",
        "from sdv.single_table import CTGANSynthesizer\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "# Load data\n",
        "rna_data = pd.read_csv('mGE.csv')\n",
        "dna_data = pd.read_csv('mDM.csv')\n",
        "cna_data = pd.read_csv('mCNA.csv')\n",
        "\n",
        "# Find common IDs\n",
        "common_ids = set(rna_data['SAMPLE_ID']).intersection(dna_data['SAMPLE_ID'], cna_data['SAMPLE_ID'])\n",
        "\n",
        "# Filter data to include only common samples\n",
        "rna_data = rna_data[rna_data['SAMPLE_ID'].isin(common_ids)]\n",
        "dna_data = dna_data[dna_data['SAMPLE_ID'].isin(common_ids)]\n",
        "cna_data = cna_data[cna_data['SAMPLE_ID'].isin(common_ids)]\n",
        "\n",
        "# Extract features and class labels\n",
        "X_train_rna = rna_data.iloc[:, 1:].values\n",
        "X_train_dna = dna_data.iloc[:, 1:].values\n",
        "X_train_cna = cna_data.iloc[:, 1:].values\n",
        "y_train = rna_data['CLASS'].values\n",
        "\n",
        "# --- 2. Autoencoder Training ---\n",
        "def build_autoencoder(input_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_dim=input_dim))\n",
        "    model.add(layers.Dense(input_dim, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Create and train autoencoders (adjust epochs and batch_size as needed)\n",
        "autoencoder_rna = build_autoencoder(X_train_rna.shape[1])\n",
        "autoencoder_rna.fit(X_train_rna, X_train_rna, epochs=30, batch_size=64, validation_split=0.2)\n",
        "\n",
        "autoencoder_dna = build_autoencoder(X_train_dna.shape[1])\n",
        "autoencoder_dna.fit(X_train_dna, X_train_dna, epochs=30, batch_size=64, validation_split=0.2)\n",
        "\n",
        "autoencoder_cna = build_autoencoder(X_train_cna.shape[1])\n",
        "autoencoder_cna.fit(X_train_cna, X_train_cna, epochs=30, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Extract latent space representations\n",
        "latent_rna = autoencoder_rna.predict(X_train_rna)\n",
        "latent_dna = autoencoder_dna.predict(X_train_dna)\n",
        "latent_cna = autoencoder_cna.predict(X_train_cna)\n",
        "\n",
        "# Concatenate latent spaces\n",
        "latent_space = np.concatenate((latent_rna, latent_dna, latent_cna), axis=1)\n",
        "\n",
        "# --- 3. Data Augmentation with CTGAN ---\n",
        "# Automatically determine the minority class\n",
        "class_counts = np.unique(y_train, return_counts=True)\n",
        "minority_class = class_counts[0][np.argmin(class_counts[1])]\n",
        "\n",
        "# Separate majority and minority class samples\n",
        "majority_samples = latent_space[y_train != minority_class]\n",
        "minority_samples = latent_space[y_train == minority_class]\n",
        "\n",
        "\n",
        "# Fit CTGAN to the minority class data in the LATENT SPACE\n",
        "minority_data_latent = pd.DataFrame(minority_samples)\n",
        "\n",
        "# Create metadata and define NO primary key - latent space has no id\n",
        "metadata = SingleTableMetadata()\n",
        "metadata.detect_from_dataframe(data=minority_data_latent)\n",
        "\n",
        "# Create a CTGAN synthesizer\n",
        "ctgan = CTGANSynthesizer(metadata=metadata,\n",
        "                         epochs=300,        # Adjust as needed\n",
        "                         batch_size=500     # Adjust as needed\n",
        ")\n",
        "\n",
        "ctgan.fit(minority_data_latent)\n",
        "\n",
        "# Generate synthetic samples\n",
        "num_synthetic_samples = len(majority_samples)  # Generate as many as the majority class\n",
        "synthetic_data = ctgan.sample(num_synthetic_samples)\n",
        "\n",
        "\n",
        "# Combine synthetic samples with original data\n",
        "augmented_data = np.vstack([latent_space, synthetic_data])\n",
        "# Modify augmented_labels to use 0 for the original data and 1 for the synthetic data\n",
        "augmented_labels = np.concatenate([np.zeros(len(y_train)), np.ones(num_synthetic_samples)])\n",
        "\n",
        "# Shuffle augmented data\n",
        "shuffle_idx = np.random.permutation(len(augmented_data))\n",
        "augmented_data = augmented_data[shuffle_idx]\n",
        "augmented_labels = augmented_labels[shuffle_idx]\n",
        "\n",
        "# --- 4. Cross-Validation and Evaluation ---\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "cv_results = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'accuracy': [],\n",
        "    'auc_roc': [],\n",
        "    'fpr': [],\n",
        "    'tpr': []\n",
        "}\n",
        "\n",
        "for train_index, test_index in kf.split(augmented_data):\n",
        "    X_train_eval, X_test_eval = augmented_data[train_index], augmented_data[test_index]\n",
        "    y_train_eval, y_test_eval = augmented_labels[train_index], augmented_labels[test_index]\n",
        "\n",
        "    # Define and compile the prediction model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(128, activation='relu', input_dim=X_train_eval.shape[1]))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Shuffle the target variable\n",
        "    # y_train_shuffled = np.random.permutation(y_train_eval)\n",
        "\n",
        "    #history = model.fit(X_train_eval, y_train_shuffled, epochs=30, batch_size=32,\n",
        "    #                    validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train_eval, y_train_eval, epochs=30, batch_size=32,\n",
        "                        validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "    # Store metrics\n",
        "    cv_results['train_loss'].append(history.history['loss'])\n",
        "    cv_results['val_loss'].append(history.history['val_loss'])\n",
        "    cv_results['train_acc'].append(history.history['accuracy'])\n",
        "    cv_results['val_acc'].append(history.history['val_accuracy'])\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred_eval = model.predict(X_test_eval)\n",
        "    y_pred_binary_eval = (y_pred_eval > 0.5).astype(int)\n",
        "\n",
        "    # Calculate and store evaluation metrics\n",
        "    accuracy_eval = accuracy_score(y_test_eval, y_pred_binary_eval)\n",
        "    auc_roc_eval = roc_auc_score(y_test_eval, y_pred_eval)\n",
        "\n",
        "    # Move these lines *inside* the loop\n",
        "    fpr, tpr, _ = roc_curve(y_test_eval, y_pred_eval)\n",
        "    cv_results['accuracy'].append(accuracy_eval)\n",
        "    cv_results['auc_roc'].append(auc_roc_eval)\n",
        "    cv_results['fpr'].append(fpr)\n",
        "    cv_results['tpr'].append(tpr)\n",
        "\n",
        "# --- Calculate and Print Average Metrics ---\n",
        "avg_train_loss = np.mean([np.min(losses) for losses in cv_results['train_loss']])\n",
        "avg_val_loss = np.mean([np.min(losses) for losses in cv_results['val_loss']])\n",
        "avg_train_acc = np.mean([np.max(accs) for accs in cv_results['train_acc']])\n",
        "avg_val_acc = np.mean([np.max(accs) for accs in cv_results['val_acc']])\n",
        "avg_accuracy = np.mean(cv_results['accuracy'])\n",
        "avg_auc_roc = np.mean(cv_results['auc_roc'])\n",
        "\n",
        "print(f'Average Training Loss: {avg_train_loss}')\n",
        "print(f'Average Validation Loss: {avg_val_loss}')\n",
        "print(f'Average Training Accuracy: {avg_train_acc}')\n",
        "print(f'Average Validation Accuracy: {avg_val_acc}')\n",
        "print(f'Average Accuracy: {avg_accuracy}')\n",
        "print(f'Average AUC-ROC: {avg_auc_roc}')\n",
        "\n",
        "# --- Generate Plots ---\n",
        "# Plot training and validation loss for the last fold\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(cv_results['train_loss'][-1]) + 1), cv_results['train_loss'][-1], label='Training Loss')\n",
        "plt.plot(range(1, len(cv_results['val_loss'][-1]) + 1), cv_results['val_loss'][-1], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss (Last Fold)')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation accuracy for the last fold\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(cv_results['train_acc'][-1]) + 1), cv_results['train_acc'][-1], label='Training Accuracy')\n",
        "plt.plot(range(1, len(cv_results['val_acc'][-1]) + 1), cv_results['val_acc'][-1], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy (Last Fold)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Combine FPR and TPR from all folds for a single ROC curve with more points\n",
        "all_fpr = np.unique(np.concatenate([cv_results['fpr'][i] for i in range(kf.n_splits)]))\n",
        "\n",
        "# Interpolate to get more points on the ROC curve\n",
        "num_points = 100  # You can adjust this number for more/fewer points\n",
        "base_fpr = np.linspace(0, 1, num_points)\n",
        "\n",
        "mean_tpr = np.zeros_like(base_fpr)\n",
        "for i in range(kf.n_splits):\n",
        "    mean_tpr += np.interp(base_fpr, cv_results['fpr'][i], cv_results['tpr'][i])\n",
        "mean_tpr /= kf.n_splits\n",
        "\n",
        "roc_auc = auc(base_fpr, mean_tpr) # Calculate AUC using interpolated points\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(base_fpr, mean_tpr, color='darkorange', lw=2, label='Mean ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vCkF48_pyDg-"
      },
      "id": "vCkF48_pyDg-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Combine FPR and TPR from all folds for a single ROC curve with more points\n",
        "all_fpr = np.unique(np.concatenate([cv_results['fpr'][i] for i in range(kf.n_splits)]))\n",
        "\n",
        "# Interpolate to get more points on the ROC curve\n",
        "num_points = 100  # You can adjust this number for more/fewer points\n",
        "base_fpr = np.linspace(0, 1, num_points)\n",
        "\n",
        "mean_tpr = np.zeros_like(base_fpr)\n",
        "for i in range(kf.n_splits):\n",
        "    mean_tpr += np.interp(base_fpr, cv_results['fpr'][i], cv_results['tpr'][i])\n",
        "mean_tpr /= kf.n_splits\n",
        "\n",
        "# --- Add (0, 0) to ROC data ---\n",
        "base_fpr = np.concatenate(([0], base_fpr))  # Add 0 to FPR\n",
        "mean_tpr = np.concatenate(([0], mean_tpr))  # Add 0 to TPR\n",
        "\n",
        "\n",
        "roc_auc = auc(base_fpr, mean_tpr) # Calculate AUC using interpolated points\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(base_fpr, mean_tpr, color='darkorange', lw=2, label='Mean ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jrxCcL09yzST"
      },
      "id": "jrxCcL09yzST",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8x3l-1tL1VVL"
      },
      "id": "8x3l-1tL1VVL"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# ... (after training your model and making predictions) ...\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_eval = model.predict(X_test_eval)\n",
        "y_pred_binary_eval = (y_pred_eval > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate and print the confusion matrix\n",
        "cm = confusion_matrix(y_test_eval, y_pred_binary_eval)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "ALL8OjNz5I9y"
      },
      "id": "ALL8OjNz5I9y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# ... (after training your model and making predictions) ...\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_eval = model.predict(X_test_eval)\n",
        "y_pred_binary_eval = (y_pred_eval > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test_eval, y_pred_binary_eval)\n",
        "\n",
        "# Extract TP, FN, FP, TN\n",
        "TP = cm[1, 1]  # True Positives\n",
        "FN = cm[1, 0]  # False Negatives\n",
        "FP = cm[0, 1]  # False Positives\n",
        "TN = cm[0, 0]  # True Negatives\n",
        "\n",
        "# Print in the desired format\n",
        "print(f\"{TP} {FN}\")\n",
        "print(f\"{FP} {TN}\")"
      ],
      "metadata": {
        "id": "BH_yEXZl5cw4"
      },
      "id": "BH_yEXZl5cw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# ... (after training your model and making predictions) ...\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_eval = model.predict(X_test_eval)\n",
        "y_pred_binary_eval = (y_pred_eval > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test_eval, y_pred_binary_eval)\n",
        "\n",
        "# Extract TP, FN, FP, TN\n",
        "TP = cm[1, 1]  # True Positives\n",
        "FN = cm[1, 0]  # False Negatives\n",
        "FP = cm[0, 1]  # False Positives\n",
        "TN = cm[0, 0]  # True Negatives\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_eval, y_pred_binary_eval)\n",
        "\n",
        "# Print in the desired format\n",
        "print(f\"TP: {TP}\")\n",
        "print(f\"FN: {FN}\")\n",
        "print(f\"FP: {FP}\")\n",
        "print(f\"TN: {TN}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "aCfBfEFr5p5Y"
      },
      "id": "aCfBfEFr5p5Y",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}